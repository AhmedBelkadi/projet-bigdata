# =============================================================================
# Flume Agent Configuration: Exec Source -> Memory Channel -> HDFS Sink
# =============================================================================
# Ingests log lines from log_generator.py (exec source) and writes them to
# HDFS at /user/hadoopuser/project/logs/. Run with:
#   flume-ng agent -n agent -c conf -f /path/to/agent.conf
# Ensure HDFS is up and fs.defaultFS (e.g. hdfs://namenode:9000) is set.
# =============================================================================

# -----------------------------------------------------------------------------
# Agent name and component list
# -----------------------------------------------------------------------------
# This agent has one source (exec), one channel (memory), one sink (hdfs).
# Names are arbitrary; wire them in the "Bind" section below.
agent.sources = exec-src
agent.channels = mem-ch
agent.sinks = hdfs-sink

# -----------------------------------------------------------------------------
# Bind source and sink to the channel
# -----------------------------------------------------------------------------
agent.sources.exec-src.channels = mem-ch
agent.sinks.hdfs-sink.channel = mem-ch

# =============================================================================
# EXEC SOURCE — runs a command and treats each line of stdout as one event
# =============================================================================
agent.sources.exec-src.type = exec

# Command to run. log_generator.py streams JSON log lines to stdout at 5/sec.
# Use full path if Python/log_generator.py are not in PATH when Flume runs.
agent.sources.exec-src.command = python3 /project/log_generator.py --rate 5

# Optional: shell used to run the command (e.g. /bin/bash -c "command").
# Uncomment if you need shell expansion or a different interpreter path.
# agent.sources.exec-src.shell = /bin/bash -c

# Restart the command if it exits (e.g. script crash). Recommended for
# long-running generators so the agent recovers without manual restart.
agent.sources.exec-src.restart = true
agent.sources.exec-src.restartThrottle = 10000

# Maximum number of lines to read per batch (default 100). Tune if needed.
agent.sources.exec-src.batchSize = 100

# =============================================================================
# MEMORY CHANNEL — in-memory queue between source and sink
# =============================================================================
agent.channels.mem-ch.type = memory

# Maximum number of events the channel can hold. Buffers bursts when HDFS
# is temporarily slow. 10000 = up to 10000 log lines in memory.
agent.channels.mem-ch.capacity = 10000

# Number of events per transaction (put/take). Should be <= capacity.
# Larger values reduce transaction overhead; match or exceed sink batch size.
agent.channels.mem-ch.transactionCapacity = 1000

# Optional: keep-alive for pollable sources (not used by exec source).
# agent.channels.mem-ch.keep-alive = 3

# =============================================================================
# HDFS SINK — writes events to HDFS with rolling files
# =============================================================================
agent.sinks.hdfs-sink.type = hdfs

# Destination directory in HDFS. Use path consistent with cluster fs.defaultFS
# (e.g. hdfs://namenode:9000). Optional: add time partition, e.g.:
#   /user/hadoopuser/project/logs/%Y-%m-%d
# so files are organized by date.
agent.sinks.hdfs-sink.hdfs.path = hdfs://namenode:9000/user/hadoopuser/project/logs/

# Write format: one event per line (no Avro/SequenceFile). Text/DataStream.
agent.sinks.hdfs-sink.hdfs.fileType = DataStream

# File naming: prefix and suffix. Flume appends a timestamp to avoid clashes.
# Resulting files look like: app-logs-.1739620800000.log
agent.sinks.hdfs-sink.hdfs.filePrefix = app-logs-
agent.sinks.hdfs-sink.hdfs.fileSuffix = .log

# --- Roll policy: close current file and open a new one when ANY condition is met ---
# Roll every 60 seconds (time-based batching).
agent.sinks.hdfs-sink.hdfs.rollInterval = 60

# Roll when file size reaches 10 MB (10 * 1024 * 1024 bytes).
agent.sinks.hdfs-sink.hdfs.rollSize = 10485760

# Roll after 1000 events written to the current file.
agent.sinks.hdfs-sink.hdfs.rollCount = 1000

# Round roll interval down to 0 (no rounding). Optional.
# agent.sinks.hdfs-sink.hdfs.round = false

# Number of seconds to wait before closing an idle file (no new events).
agent.sinks.hdfs-sink.hdfs.idleTimeout = 0

# Batch size: how many events the sink takes from the channel per transaction.
agent.sinks.hdfs-sink.hdfs.batchSize = 1000

# Use HDFS write pipeline (default). Set to 0 to disable for debugging.
agent.sinks.hdfs-sink.hdfs.minBlockReplicas = 1

# Allow overwrite of existing files (default false). Keep false for append-style logs.
# agent.sinks.hdfs-sink.hdfs.allowAppend = false
